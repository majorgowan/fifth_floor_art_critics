{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Processing data to run PCA and then NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd ../Python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import readPBNData.description as rd\n",
    "import readPBNData.images as ri"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvs, fileLike = rd.openZip('../Data/train_info.csv.zip')\n",
    "lines, head = rd.readCSV(fileLike[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(head)\n",
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = rd.columns(lines,head)\n",
    "cols.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colsdf=pd.DataFrame(cols)\n",
    "colsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols['title'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = ri.openZipImage('../Data/train_1.zip',cols['filename'][0],prefix='train_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new2= img.resize(size=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(new2.getdata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new= img.resize(size=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(new.format, new.size, new.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#r, g, b = new.split()\n",
    "#r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "zf = zipfile.ZipFile('../Data/train_1.zip','r')\n",
    "filenames = zf.namelist()[1:]\n",
    "filenames[:10]\n",
    "#print([os.path.basename(h) for h in filenames])\n",
    "\n",
    "#os.path.basename(filenames[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import readPBNData.description as rd\n",
    "import readPBNData.images as ri\n",
    "from PIL import Image, ImageMath\n",
    "import numpy as np\n",
    "\n",
    "mylist=[]\n",
    "invalid = pd.DataFrame()\n",
    "for h in range(2000):\n",
    "    #del img, new, cleanlist\n",
    "    img = ri.openZipImage('../Data/train_1.zip',filenames[h])\n",
    "    try:\n",
    "        new= img.resize(size=(100,100))\n",
    "    except IOError as e:\n",
    "        invalid = invalid.append([[filenames[h], e]],ignore_index=True)\n",
    "    cleanlist=np.array(new).flatten()\n",
    "    mylist.append([cleanlist,os.path.basename(filenames[h])])\n",
    "mylist=np.array(mylist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mylist[1200:1300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mylist2[:,0][0]\n",
    "df = pd.DataFrame({'pixels':mylist[:,0],'filename':mylist[:,1]})\n",
    "#mylist[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.merge(df, colsdf, on='filename')\n",
    "#result = pd.concat([df, colsdf], axis=1, join_axes=[df.filename])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = result.groupby('artist')\n",
    "np.sort(grouped.size())[-200:]\n",
    "#np.sort(grouped.size())[-50:]\n",
    "#grouped.size().sort_values(axis=0, ascending=True)\n",
    "#result.groupby('artist', sort=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grouped.groups\n",
    "final=grouped.filter(lambda x: len(x) > 9)\n",
    "final\n",
    "#result.artist[np.sort(grouped.size())[-100:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create classes based on artist (only 24 artist in this sample, the most frequent ones)\n",
    "uniq = sorted(list(set(final['artist'])))\n",
    "artnum = [uniq.index(x) for x in final['artist']]\n",
    "# Put the data in the right format to run the NN\n",
    "frame2=[pd.DataFrame(final['pixels'].tolist()),pd.DataFrame(artnum)]\n",
    "datafinal=pd.concat(frame2, axis=1).dropna(axis=0)\n",
    "datafinal.columns = [list(range(30001))]\n",
    "\n",
    "y_class = datafinal[30000]\n",
    "X_train = datafinal.ix[:,0:29999]\n",
    "y_train = y_class\n",
    "#X_test = dtm[n_break:n_final]\n",
    "#y_test = y_class[n_break:n_final]\n",
    "print(X_train.shape,y_train.shape,type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#final['pixels'].tolist()\n",
    "#pd.DataFrame(data=final['pixels'].tolist())\n",
    "#final['artist']\n",
    "#X_train\n",
    "#pd.isnull(X_train[29990])\n",
    "#X_train.dropna(axis=0).shape\n",
    "#datafinal=pd.DataFrame({final['pixels'].tolist(),artnum})\n",
    "#frame2=[pd.DataFrame(final['pixels'].tolist()),pd.DataFrame(artnum)]\n",
    "#datafinal=pd.concat(frame2, axis=1).dropna(axis=0)\n",
    "#datafinal.columns = [list(range(30001))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(final['pixels'].tolist())\n",
    "#pd.DataFrame(artnum).transpose()\n",
    "#datafinal[30000]\n",
    "#range(10)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis to MOvie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Principal Component Analysis to MOvie data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, RandomizedPCA\n",
    "from sklearn.preprocessing import scale\n",
    "%matplotlib inline\n",
    "# Note that I use scale and then PCA with Whitening, because whitening eliminates\n",
    "# correlation among features but scaling only affects the features independently\n",
    "components= 270\n",
    "pca = PCA( n_components = components,whiten=True )\n",
    "#pca = RandomizedPCA(n_components = 200, random_state=0 )\n",
    "X_pca = pca.fit_transform(scale(X_train))\n",
    "#X_pca_test = pca.transform(scale(X_test))\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first n components): %s'\n",
    "      % str(sum(pca.explained_variance_ratio_)))\n",
    "print(X_pca.shape)\n",
    "\n",
    "#eigenvalues = pca.explained_variance_\n",
    "plt.figure(figsize=(4, 4))\n",
    "eigencumul=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "plt.plot(range(components),eigencumul)\n",
    "\n",
    "plt.ylim(0, 110)\n",
    "plt.xlim(0, components)\n",
    "#plt.ylabel('Error')\n",
    "plt.xlabel('# of components')\n",
    "plt.title('Image Dataset \\n Cumulative percentage of variance \\n explained by components \\n', fontsize=12, ha='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Applying a NN using Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import matplotlib\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "#import pydot_ng\n",
    "#import pydot\n",
    "from IPython.display import Image\n",
    "from IPython.display import SVG\n",
    "from sklearn.preprocessing import scale\n",
    "import timeit\n",
    "from time import time\n",
    "\n",
    "# Display plots inline and change default figure size\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "\n",
    "# Note to TAs: you have to un comment the paragrah below to calculate results for\n",
    "# the different variables\n",
    "\n",
    "##Movie data\n",
    "\n",
    "## Original data\n",
    "train_y = y_train.astype(np.int32)\n",
    "#test_y = y_test.astype(np.int32)\n",
    "train_X = scale(X_train).astype(np.float32)\n",
    "#test_X = scale(X_test).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Size definitions\n",
    "num_examples = len(train_X) # training set size\n",
    "nn_input_dim = train_X.shape[1] # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality (2 for Movie data and 4 for Wine data)\n",
    "nn_hdim = 3 # hiden layer dimensionality\n",
    "\n",
    "# Gradient descent parameters \n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength \n",
    "\n",
    "# Our data vectors\n",
    "X = T.matrix('X') # matrix of doubles\n",
    "y = T.lvector('y') # vector of int64\n",
    "\n",
    "# Shared variables with initial values. We need to learn these.\n",
    "W1 = theano.shared(np.random.randn(nn_input_dim, nn_hdim), name='W1')\n",
    "b1 = theano.shared(np.zeros(nn_hdim), name='b1')\n",
    "W2 = theano.shared(np.random.randn(nn_hdim, nn_output_dim), name='W2')\n",
    "b2 = theano.shared(np.zeros(nn_output_dim), name='b2')\n",
    "\n",
    "# Forward propagation\n",
    "# Note: We are just defining the expressions, nothing is evaluated here!\n",
    "z1 = X.dot(W1) + b1\n",
    "a1 = T.tanh(z1)\n",
    "z2 = a1.dot(W2) + b2\n",
    "y_hat = T.nnet.softmax(z2) # output probabilties\n",
    "\n",
    "# The regularization term (optional)\n",
    "loss_reg = 1./num_examples * reg_lambda/2 * (T.sum(T.sqr(W1)) + T.sum(T.sqr(W2))) \n",
    "# the loss function we want to optimize\n",
    "loss = T.nnet.categorical_crossentropy(y_hat, y).mean() + loss_reg\n",
    "\n",
    "# Returns a class prediction\n",
    "prediction = T.argmax(y_hat, axis=1)\n",
    "\n",
    "# Theano functions that can be called from our Python code\n",
    "forward_prop = theano.function([X], y_hat)\n",
    "calculate_loss = theano.function([X, y], loss)\n",
    "predict = theano.function([X], prediction)\n",
    "\n",
    "# Easy: Let Theano calculate the derivatives for us!\n",
    "dW2 = T.grad(loss, W2)\n",
    "db2 = T.grad(loss, b2)\n",
    "dW1 = T.grad(loss, W1)\n",
    "db1 = T.grad(loss, b1)\n",
    "\n",
    "gradient_step = theano.function(\n",
    "    [X, y],\n",
    "    updates=((W2, W2 - epsilon * dW2),\n",
    "             (W1, W1 - epsilon * dW1),\n",
    "             (b2, b2 - epsilon * db2),\n",
    "             (b1, b1 - epsilon * db1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function learns parameters for the neural network and returns the model.\n",
    "# - num_passes: Number of passes through the training data for gradient descent\n",
    "# - print_loss: If True, print the loss every 100 iterations\n",
    "train_errorsNN = []\n",
    "test_errorsNN = []\n",
    "num_passes= 2000\n",
    "def build_model(num_passes=num_passes, print_loss=False):\n",
    "    \n",
    "    # Re-Initialize the parameters to random values. We need to learn these.\n",
    "    # (Needed in case we call this function multiple times)\n",
    "    np.random.seed(0)\n",
    "    W1.set_value(np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim))\n",
    "    b1.set_value(np.zeros(nn_hdim))\n",
    "    W2.set_value(np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim))\n",
    "    b2.set_value(np.zeros(nn_output_dim))\n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, num_passes):\n",
    "        # This will update our parameters W2, b2, W1 and b1!\n",
    "        gradient_step(train_X, train_y)\n",
    "        \n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 100 == 0:\n",
    "            #print(\"Loss after iteration %i: %f\" %(i, calculate_loss(train_X, train_y)))\n",
    "            train_errorsNN.append(1. - np.mean(predict(train_X)==train_y))\n",
    "            #test_errorsNN.append(1. - np.mean(predict(test_X)==test_y))\n",
    "  \n",
    "# Build a model with a n-dimensional hidden layer\n",
    "\n",
    "t0 = time()\n",
    "build_model(print_loss=True)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Plot the decision boundary\n",
    "#plot_decision_boundary(lambda x: predict(x))\n",
    "#plt.title(\"Decision Boundary for hidden layer size 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Just trying stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert image to array , and then from array to image\n",
    "im = np.array(new)\n",
    "#im.flatten().reshape(100,100,3)\n",
    "im3 = Image.fromarray(im.flatten().reshape(100,100,3))\n",
    "im3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = array(new)\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "immatrix\n",
    "imlist=[new, new]\n",
    "len(imlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imlist=[array(new).reshape(-1, 3), array(new).reshape(-1, 3)]\n",
    "len(imlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#im.reshape(-1, 3).shape\n",
    "\n",
    "imlist=[array(new).reshape(-1, 3), array(new).reshape(-1, 3)]\n",
    "im = imlist[0] # open one image to get size\n",
    "m,n = im.shape[0:2] # get the size of the images\n",
    "imnbr = len(imlist) # get the number of images\n",
    "\n",
    "# create matrix to store all flattened images\n",
    "#immatrix = array([array(Image.open(im)).flatten()\n",
    "              #for j in imlist],'f')\n",
    "\n",
    "# create matrix to store all flattened images\n",
    "immatrix = array([imlist[i].flatten()\n",
    "              for i in range(len(imlist))],'f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imshow(immean.reshape(m,n,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "immatrix.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca2(immatrix)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V[1].reshape(m,n,r).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "immatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_X = X.mean(axis=0)\n",
    "mean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "immatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=immatrix\n",
    "M = dot(X,X.T)\n",
    "linalg.eigh(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}